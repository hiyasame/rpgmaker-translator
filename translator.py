#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import json
import re
import time
import os
import glob
import asyncio
import threading
import shutil
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Any, Dict, List, Tuple
import argparse

# å¯¼å…¥Gemini
try:
    import google.generativeai as genai
    GEMINI_AVAILABLE = True
except ImportError:
    GEMINI_AVAILABLE = False
    print("âŒ é”™è¯¯: è¯·å®‰è£…Geminiä¾èµ–: pip install google-generativeai")
    exit(1)

class RateLimiter:
    """ç®€å•çš„é€Ÿç‡é™åˆ¶å™¨"""
    def __init__(self, max_requests_per_minute: int = 60):
        self.max_requests = max_requests_per_minute
        self.requests = []
        self.lock = threading.Lock()
    
    def wait_if_needed(self):
        """å¦‚æœéœ€è¦åˆ™ç­‰å¾…ä»¥éµå®ˆé€Ÿç‡é™åˆ¶"""
        with self.lock:
            now = time.time()
            
            # æ¸…ç†1åˆ†é’Ÿå‰çš„è¯·æ±‚è®°å½•
            self.requests = [req_time for req_time in self.requests if now - req_time < 60]
            
            # å¦‚æœè¾¾åˆ°é™åˆ¶ï¼Œç­‰å¾…åˆ°æœ€æ—©è¯·æ±‚è¿‡æœŸ
            if len(self.requests) >= self.max_requests:
                sleep_time = 60 - (now - self.requests[0]) + 0.1  # å¤šç­‰0.1ç§’ä¿é™©
                if sleep_time > 0:
                    print(f"â±ï¸ è¾¾åˆ°é€Ÿç‡é™åˆ¶ï¼Œç­‰å¾… {sleep_time:.1f}s...")
                    time.sleep(sleep_time)
                    # é‡æ–°æ¸…ç†
                    now = time.time()
                    self.requests = [req_time for req_time in self.requests if now - req_time < 60]
            
            # è®°å½•è¿™æ¬¡è¯·æ±‚
            self.requests.append(now)

class JSONJapaneseLocalizer:
    def __init__(self, gemini_api_key: str, gemini_base_url: str = "https://api.openai-proxy.org/google",
                 gemini_model: str = "gemini-2.5-flash", batch_size: int = 20, max_workers: int = 3,
                 requests_per_minute: int = 60):
        
        if not gemini_api_key:
            raise ValueError("å¿…é¡»æä¾›Gemini APIå¯†é’¥")
        
        self.batch_size = batch_size
        self.max_workers = max_workers
        self.rate_limiter = RateLimiter(requests_per_minute)
        
        # é…ç½®Gemini
        genai.configure(
            api_key=gemini_api_key,
            transport="rest",
            client_options={"api_endpoint": gemini_base_url}
        )
        self.genai_model = genai.GenerativeModel(gemini_model)
        print(f"âœ… å·²åˆå§‹åŒ–Geminiæ¨¡å‹: {gemini_model}")
        print(f"ğŸ“¦ æ‰¹é‡å¤§å°: {batch_size}, å¹¶å‘æ•°: {max_workers}, é€Ÿç‡é™åˆ¶: {requests_per_minute} RPM")
    
    def is_japanese(self, text: str) -> bool:
        """æ£€æµ‹æ–‡æœ¬æ˜¯å¦åŒ…å«æ—¥æ–‡å­—ç¬¦"""
        if not isinstance(text, str) or not text.strip():
            return False
        
        # æ—¥æ–‡å­—ç¬¦èŒƒå›´ï¼šå¹³å‡åã€ç‰‡å‡åã€æ±‰å­—
        japanese_pattern = re.compile(r'[\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FAF]')
        return bool(japanese_pattern.search(text))
    
    def should_skip_translation(self, path: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥è·³è¿‡ç¿»è¯‘ï¼ˆå¦‚æœè·¯å¾„åŒ…å«bgmæˆ–bgsï¼‰"""
        path_lower = path.lower()
        return 'bgm' in path_lower or 'bgs' in path_lower
    
    def collect_japanese_texts(self, data: Any, texts: List[Tuple[str, str]] = None, path: str = "") -> List[Tuple[str, str]]:
        """æ”¶é›†æ‰€æœ‰æ—¥æ–‡å­—ç¬¦ä¸²åŠå…¶ä½ç½®è·¯å¾„"""
        if texts is None:
            texts = []
        
        if isinstance(data, str):
            if self.is_japanese(data) and not self.should_skip_translation(path):
                texts.append((path, data))
        elif isinstance(data, dict):
            for key, value in data.items():
                new_path = f"{path}.{key}" if path else key
                self.collect_japanese_texts(value, texts, new_path)
        elif isinstance(data, list):
            for i, item in enumerate(data):
                new_path = f"{path}[{i}]"
                self.collect_japanese_texts(item, texts, new_path)
        
        return texts
    
    def translate_batch_with_gemini(self, texts: List[str], batch_id: int = 0, max_retries: int = 3) -> List[str]:
        """ä½¿ç”¨Geminiæ‰¹é‡ç¿»è¯‘æ–‡æœ¬ï¼ˆæ”¯æŒå¹¶å‘è°ƒç”¨ï¼‰"""
        if not texts:
            return []
        
        for retry in range(max_retries + 1):
            # ç­‰å¾…é€Ÿç‡é™åˆ¶
            self.rate_limiter.wait_if_needed()
            
            # æ„å»ºç¼–å·æ–‡æœ¬
            numbered_text = "\n".join([f"{i+1}. {text}" for i, text in enumerate(texts)])
            
            prompt = f"""è¯·å°†ä»¥ä¸‹æ—¥æ–‡æ–‡æœ¬ç¿»è¯‘æˆä¸­æ–‡ã€‚

è¦æ±‚ï¼š
1. ä¿æŒç¼–å·æ ¼å¼ï¼ˆ1. 2. 3. ...ï¼‰
2. é€è¡Œç¿»è¯‘ï¼Œæ¯è¡Œå¯¹åº”ä¸€ä¸ªç¼–å·
3. ç¿»è¯‘è¦å‡†ç¡®è‡ªç„¶
4. åªè¿”å›ç¿»è¯‘ç»“æœï¼Œä¸è¦æ·»åŠ å…¶ä»–è¯´æ˜
5. å¿…é¡»è¿”å›{len(texts)}è¡Œç¿»è¯‘ç»“æœ

å¾…ç¿»è¯‘æ–‡æœ¬ï¼š
{numbered_text}

ä¸­æ–‡ç¿»è¯‘ï¼š"""

            try:
                start_time = time.time()
                response = self.genai_model.generate_content(prompt)
                duration = time.time() - start_time
                
                if response.text:
                    result = self.parse_gemini_response(response.text.strip(), texts, batch_id)
                    
                    # æ£€æŸ¥è¿”å›çš„ç¿»è¯‘æ•°é‡æ˜¯å¦åŒ¹é…
                    if len(result) == len(texts):
                        print(f"âœ… æ‰¹æ¬¡{batch_id} å®Œæˆ ({duration:.2f}s)")
                        return result
                    else:
                        if retry < max_retries:
                            print(f"âš ï¸ æ‰¹æ¬¡{batch_id} ç¬¬{retry+1}æ¬¡å°è¯•ï¼šè¿”å›{len(result)}æ¡ï¼ŒæœŸæœ›{len(texts)}æ¡ï¼Œé‡è¯•ä¸­...")
                            continue
                        else:
                            print(f"âŒ æ‰¹æ¬¡{batch_id} é‡è¯•{max_retries}æ¬¡åä»ä¸åŒ¹é…ï¼Œä¿ç•™åŸæ–‡")
                            return texts
                else:
                    if retry < max_retries:
                        print(f"âš ï¸ æ‰¹æ¬¡{batch_id} ç¬¬{retry+1}æ¬¡å°è¯•ï¼šGeminiè¿”å›ç©ºå“åº”ï¼Œé‡è¯•ä¸­...")
                        continue
                    else:
                        print(f"âŒ æ‰¹æ¬¡{batch_id} é‡è¯•{max_retries}æ¬¡åä»è¿”å›ç©ºå“åº”ï¼Œä¿ç•™åŸæ–‡")
                        return texts
                        
            except Exception as e:
                if retry < max_retries:
                    print(f"âš ï¸ æ‰¹æ¬¡{batch_id} ç¬¬{retry+1}æ¬¡å°è¯•å¤±è´¥: {e}ï¼Œé‡è¯•ä¸­...")
                    time.sleep(2)  # é”™è¯¯åç­‰å¾…2ç§’å†é‡è¯•
                    continue
                else:
                    print(f"âŒ æ‰¹æ¬¡{batch_id} é‡è¯•{max_retries}æ¬¡åä»å¤±è´¥: {e}ï¼Œä¿ç•™åŸæ–‡")
                    return texts
        
        return texts  # ä¸åº”è¯¥åˆ°è¾¾è¿™é‡Œï¼Œä½†ä¿é™©èµ·è§
    
    def parse_gemini_response(self, response_text: str, original_texts: List[str], batch_id: int = 0) -> List[str]:
        """è§£æGeminiçš„ç¿»è¯‘å“åº”"""
        lines = response_text.split('\n')
        translations = []
        
        # å°è¯•è§£æç¼–å·æ ¼å¼
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # åŒ¹é… "æ•°å­—. å†…å®¹" æ ¼å¼
            match = re.match(r'^\d+\.\s*(.+)', line)
            if match:
                translations.append(match.group(1))
        
        # å¦‚æœè§£ææ•°é‡ä¸å¯¹ï¼Œå°è¯•ç®€å•åˆ†å‰²
        if len(translations) != len(original_texts):
            print(f"âš ï¸ æ‰¹æ¬¡{batch_id} è§£æå¼‚å¸¸ï¼ŒæœŸæœ›{len(original_texts)}è¡Œï¼Œå¾—åˆ°{len(translations)}è¡Œï¼Œé‡æ–°è§£æ...")
            
            # æ¸…ç†å’Œé‡æ–°æå–
            clean_lines = []
            skip_keywords = ['ä¸­æ–‡ç¿»è¯‘ï¼š', 'ç¿»è¯‘ç»“æœï¼š', 'ç¿»è¯‘å¦‚ä¸‹ï¼š', 'ä»¥ä¸‹æ˜¯ç¿»è¯‘ï¼š']
            
            for line in lines:
                line = line.strip()
                if not line:
                    continue
                    
                # è·³è¿‡æ ‡é¢˜è¡Œ
                if any(keyword in line for keyword in skip_keywords):
                    continue
                
                # å»æ‰ç¼–å·å‰ç¼€
                clean_line = re.sub(r'^\d+\.\s*', '', line)
                if clean_line and clean_line not in skip_keywords:
                    clean_lines.append(clean_line)
            
            # å–å‰Nè¡Œä½œä¸ºç¿»è¯‘ç»“æœ
            if len(clean_lines) >= len(original_texts):
                translations = clean_lines[:len(original_texts)]
                print(f"âœ… æ‰¹æ¬¡{batch_id} é‡æ–°è§£ææˆåŠŸï¼Œæå–{len(translations)}è¡Œç¿»è¯‘")
            else:
                print(f"âŒ æ‰¹æ¬¡{batch_id} é‡æ–°è§£æå¤±è´¥ï¼Œä¿ç•™åŸæ–‡")
                return original_texts
        
        # æ£€æŸ¥ç¿»è¯‘è´¨é‡ï¼šå¦‚æœç¿»è¯‘ç»“æœä¸åŸæ–‡å®Œå…¨ç›¸åŒï¼Œä¸ç®—ç¿»è¯‘å¤±è´¥
        final_translations = []
        for i, (original, translated) in enumerate(zip(original_texts, translations)):
            if translated and translated.strip():
                final_translations.append(translated.strip())
            else:
                print(f"âš ï¸ æ‰¹æ¬¡{batch_id} ç¬¬{i+1}è¡Œç¿»è¯‘ä¸ºç©ºï¼Œä¿ç•™åŸæ–‡")
                final_translations.append(original)
        
        return final_translations
    
    def apply_translations(self, data: Any, translation_map: Dict[str, str], path: str = "") -> Any:
        """å°†ç¿»è¯‘ç»“æœåº”ç”¨åˆ°åŸå§‹æ•°æ®ç»“æ„"""
        if isinstance(data, str):
            if path in translation_map:
                return translation_map[path]
            return data
        elif isinstance(data, dict):
            result = {}
            for key, value in data.items():
                new_path = f"{path}.{key}" if path else key
                result[key] = self.apply_translations(value, translation_map, new_path)
            return result
        elif isinstance(data, list):
            result = []
            for i, item in enumerate(data):
                new_path = f"{path}[{i}]"
                result.append(self.apply_translations(item, translation_map, new_path))
            return result
        else:
            return data
    
    def _collect_all_japanese(self, data: Any, texts: List[Tuple[str, str]] = None, path: str = "") -> List[Tuple[str, str]]:
        """æ”¶é›†æ‰€æœ‰æ—¥æ–‡å­—ç¬¦ä¸²ï¼ˆåŒ…æ‹¬bgm/bgsï¼Œç”¨äºç»Ÿè®¡ï¼‰"""
        if texts is None:
            texts = []
        
        if isinstance(data, str):
            if self.is_japanese(data):
                texts.append((path, data))
        elif isinstance(data, dict):
            for key, value in data.items():
                new_path = f"{path}.{key}" if path else key
                self._collect_all_japanese(value, texts, new_path)
        elif isinstance(data, list):
            for i, item in enumerate(data):
                new_path = f"{path}[{i}]"
                self._collect_all_japanese(item, texts, new_path)
        
        return texts

    def create_backup(self, file_path: str) -> str:
        """åˆ›å»ºæ–‡ä»¶å¤‡ä»½ï¼Œè¿”å›å¤‡ä»½æ–‡ä»¶è·¯å¾„"""
        backup_path = file_path + '.bak'
        try:
            shutil.copy2(file_path, backup_path)
            print(f"ğŸ’¾ å·²åˆ›å»ºå¤‡ä»½: {os.path.basename(backup_path)}")
            return backup_path
        except Exception as e:
            print(f"âš ï¸ åˆ›å»ºå¤‡ä»½å¤±è´¥: {e}")
            return None
    
    def translate_json_file(self, input_file: str, output_file: str = None, create_backup: bool = True):
        """ç¿»è¯‘å•ä¸ªJSONæ–‡ä»¶"""
        try:
            print(f"\nğŸ“– è¯»å–æ–‡ä»¶: {input_file}")
            with open(input_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # æ”¶é›†æ‰€æœ‰æ—¥æ–‡å­—ç¬¦ä¸²
            print("ğŸ” æ‰«ææ—¥æ–‡å­—ç¬¦ä¸²...")
            japanese_texts = self.collect_japanese_texts(data)
            
            if not japanese_texts:
                print("â„¹ï¸ æœªæ‰¾åˆ°éœ€è¦ç¿»è¯‘çš„æ—¥æ–‡å­—ç¬¦ä¸²ï¼Œè·³è¿‡è¯¥æ–‡ä»¶")
                return
            
            print(f"ğŸ“ æ‰¾åˆ° {len(japanese_texts)} ä¸ªæ—¥æ–‡å­—ç¬¦ä¸²")
            
            # æ˜¾ç¤ºè·³è¿‡çš„bgm/bgsç›¸å…³é¡¹ç›®ç»Ÿè®¡
            all_japanese = []
            self._collect_all_japanese(data, all_japanese)
            skipped_count = len(all_japanese) - len(japanese_texts)
            if skipped_count > 0:
                print(f"â­ï¸ è·³è¿‡ {skipped_count} ä¸ªbgm/bgsç›¸å…³é¡¹ç›®")
            
            # æå–æ–‡æœ¬å†…å®¹ç”¨äºç¿»è¯‘
            texts_to_translate = [text for _, text in japanese_texts]
            paths = [path for path, _ in japanese_texts]
            
            # æ˜¾ç¤ºéƒ¨åˆ†å†…å®¹é¢„è§ˆ
            print("ğŸ“‹ å¾…ç¿»è¯‘å†…å®¹é¢„è§ˆ:")
            for i, (path, text) in enumerate(japanese_texts[:3]):
                print(f"  {i+1}. {path}: {text[:50]}{'...' if len(text) > 50 else ''}")
            if len(japanese_texts) > 3:
                print(f"  ... è¿˜æœ‰ {len(japanese_texts)-3} ä¸ª")
            
            # æ‰¹é‡ç¿»è¯‘ï¼ˆå¹¶è¡Œå¤„ç†ï¼‰
            print(f"ğŸš€ å¼€å§‹å¹¶è¡Œç¿»è¯‘...")
            translation_map = {}
            
            # å‡†å¤‡æ‰¹æ¬¡ä»»åŠ¡
            batch_tasks = []
            for i in range(0, len(texts_to_translate), self.batch_size):
                batch_texts = texts_to_translate[i:i + self.batch_size]
                batch_paths = paths[i:i + self.batch_size]
                batch_id = i // self.batch_size + 1
                batch_tasks.append((batch_id, batch_texts, batch_paths))
            
            total_batches = len(batch_tasks)
            print(f"ğŸ“‹ å‡†å¤‡å¹¶è¡Œå¤„ç† {total_batches} ä¸ªæ‰¹æ¬¡ï¼Œæœ€å¤§å¹¶å‘æ•°: {self.max_workers}")
            
            # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œæ‰§è¡Œç¿»è¯‘
            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                # æäº¤æ‰€æœ‰ä»»åŠ¡
                future_to_batch = {}
                for batch_id, batch_texts, batch_paths in batch_tasks:
                    future = executor.submit(self.translate_batch_with_gemini, batch_texts, batch_id)
                    future_to_batch[future] = (batch_id, batch_texts, batch_paths)
                
                print(f"ğŸ“¤ å·²æäº¤ {len(future_to_batch)} ä¸ªç¿»è¯‘ä»»åŠ¡")
                
                # æ”¶é›†ç»“æœ
                completed_batches = 0
                for future in as_completed(future_to_batch):
                    batch_id, batch_texts, batch_paths = future_to_batch[future]
                    
                    try:
                        translated_batch = future.result()
                        completed_batches += 1
                        
                        # ä¿å­˜ç¿»è¯‘ç»“æœ
                        success_count = 0
                        for path, original, translated in zip(batch_paths, batch_texts, translated_batch):
                            translation_map[path] = translated
                            if translated != original:
                                success_count += 1
                        
                        print(f"ğŸ¯ æ‰¹æ¬¡{batch_id} å¤„ç†å®Œæˆ ({completed_batches}/{total_batches}), æˆåŠŸç¿»è¯‘ {success_count}/{len(batch_texts)} ä¸ª")
                        
                    except Exception as e:
                        print(f"âŒ æ‰¹æ¬¡{batch_id} å¤„ç†å¼‚å¸¸: {e}")
                        # å¼‚å¸¸æ—¶ä¿ç•™åŸæ–‡
                        for path, original in zip(batch_paths, batch_texts):
                            translation_map[path] = original
            
            # åº”ç”¨ç¿»è¯‘ç»“æœ
            print("ğŸ“ åº”ç”¨ç¿»è¯‘ç»“æœåˆ°JSONç»“æ„...")
            translated_data = self.apply_translations(data, translation_map)
            
            # ç¡®å®šè¾“å‡ºæ–‡ä»¶è·¯å¾„
            if output_file is None:
                # å¦‚æœæ²¡æœ‰æŒ‡å®šè¾“å‡ºæ–‡ä»¶ï¼Œåˆ™è¦†ç›–åŸæ–‡ä»¶
                output_file = input_file
                if create_backup:
                    self.create_backup(input_file)
            
            # ä¿å­˜ç¿»è¯‘åçš„æ–‡ä»¶
            print(f"ğŸ’¾ ä¿å­˜åˆ°: {output_file}")
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(translated_data, f, ensure_ascii=False, indent=2)
            
            # ç»Ÿè®¡ç»“æœ
            total_translated = sum(1 for original, translated in 
                                 zip(texts_to_translate, [translation_map[path] for path in paths])
                                 if translated != original)
            
            print(f"\nğŸ‰ æ–‡ä»¶ç¿»è¯‘å®Œæˆï¼")
            print(f"ğŸ“Š ç¿»è¯‘ç»Ÿè®¡:")
            print(f"  - æ€»å­—ç¬¦ä¸²æ•°: {len(texts_to_translate)}")
            print(f"  - ç¿»è¯‘æˆåŠŸ: {total_translated}")
            print(f"  - ä¿ç•™åŸæ–‡: {len(texts_to_translate) - total_translated}")
            print(f"  - ç¿»è¯‘ç‡: {total_translated/len(texts_to_translate)*100:.1f}%")
            
        except FileNotFoundError:
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")
        except json.JSONDecodeError:
            print(f"âŒ JSONæ ¼å¼é”™è¯¯: {input_file}")
        except Exception as e:
            print(f"âŒ å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™: {e}")
    
    def translate_directory(self, directory: str = '.', output_suffix: str = None, create_backup: bool = True):
        """ç¿»è¯‘ç›®å½•ä¸­çš„æ‰€æœ‰JSONæ–‡ä»¶"""
        # è·å–æ‰€æœ‰JSONæ–‡ä»¶
        json_files = glob.glob(os.path.join(directory, '*.json'))
        
        # è¿‡æ»¤æ‰å¤‡ä»½æ–‡ä»¶
        source_files = []
        for file in json_files:
            if not file.endswith('.bak'):
                if output_suffix:
                    # å¦‚æœæŒ‡å®šäº†åç¼€ï¼Œè¿‡æ»¤æ‰å·²ç¿»è¯‘çš„æ–‡ä»¶
                    if not file.endswith(f'{output_suffix}.json'):
                        source_files.append(file)
                else:
                    # å¦‚æœæ²¡æœ‰æŒ‡å®šåç¼€ï¼Œå¤„ç†æ‰€æœ‰éå¤‡ä»½æ–‡ä»¶
                    source_files.append(file)
        
        if not source_files:
            print(f"ğŸ“ åœ¨ç›®å½• '{directory}' ä¸­æ²¡æœ‰æ‰¾åˆ°éœ€è¦ç¿»è¯‘çš„JSONæ–‡ä»¶")
            return
        
        print(f"ğŸ¯ JSONæ—¥æ–‡ç¿»è¯‘å·¥å…·")
        print(f"ğŸ“ æ‰¾åˆ° {len(source_files)} ä¸ªJSONæ–‡ä»¶:")
        for file in source_files:
            print(f"  ğŸ“„ {file}")
        
        # é€ä¸ªç¿»è¯‘æ–‡ä»¶
        for i, input_file in enumerate(source_files, 1):
            try:
                if output_suffix:
                    # ä½¿ç”¨åç¼€æ¨¡å¼ï¼ˆç”Ÿæˆæ–°æ–‡ä»¶ï¼‰
                    base_name = os.path.splitext(input_file)[0]
                    output_file = f"{base_name}{output_suffix}.json"
                else:
                    # è¦†ç›–æ¨¡å¼ï¼ˆä½¿ç”¨å¤‡ä»½ï¼‰
                    output_file = None
                
                print(f"\n{'='*80}")
                print(f"ğŸ¯ å¤„ç†æ–‡ä»¶ {i}/{len(source_files)}: {os.path.basename(input_file)}")
                if output_file:
                    print(f"ğŸ“¤ è¾“å‡º: {os.path.basename(output_file)}")
                else:
                    print(f"ğŸ“¤ è¾“å‡º: è¦†ç›–åŸæ–‡ä»¶ï¼ˆå°†åˆ›å»ºå¤‡ä»½ï¼‰")
                print(f"{'='*80}")
                
                self.translate_json_file(input_file, output_file, create_backup)
                
            except KeyboardInterrupt:
                print(f"\nâ¹ï¸ ç”¨æˆ·ä¸­æ–­ç¿»è¯‘")
                break
            except Exception as e:
                print(f"âŒ å¤„ç†æ–‡ä»¶ {input_file} æ—¶å‡ºé”™: {e}")
                continue
        
        print(f"\nğŸ æ‰€æœ‰æ–‡ä»¶å¤„ç†å®Œæˆï¼")

def main():
    parser = argparse.ArgumentParser(description='JSONæ—¥æ–‡ç¿»è¯‘å·¥å…· - ä½¿ç”¨Gemini API')
    parser.add_argument('--api-key', required=True,
                       help='Gemini APIå¯†é’¥')
    parser.add_argument('--directory', '-d', default='.',
                       help='è¦å¤„ç†çš„ç›®å½• (é»˜è®¤: å½“å‰ç›®å½•)')
    parser.add_argument('--output-suffix', '-s', default=None,
                       help='è¾“å‡ºæ–‡ä»¶åç¼€ (å¦‚æŒ‡å®šåˆ™ç”Ÿæˆæ–°æ–‡ä»¶ï¼Œå¦åˆ™è¦†ç›–åŸæ–‡ä»¶)')
    parser.add_argument('--max-workers', '-w', type=int, default=6,
                       help='æœ€å¤§å¹¶å‘æ•° (é»˜è®¤: 6)')
    parser.add_argument('--rpm', type=int, default=100,
                       help='æ¯åˆ†é’Ÿè¯·æ±‚æ•°é™åˆ¶ (é»˜è®¤: 100, é€‚ç”¨äºgemini-2.0-flash)')
    parser.add_argument('--batch-size', '-b', type=int, default=20,
                       help='æ‰¹é‡ç¿»è¯‘å¤§å° (é»˜è®¤: 20)')
    parser.add_argument('--base-url', default='https://api.openai-proxy.org/google',
                       help='Gemini APIåŸºç¡€URL')
    parser.add_argument('--model', default='gemini-2.0-flash',
                       help='Geminiæ¨¡å‹åç§° (é»˜è®¤: gemini-2.0-flash)')
    parser.add_argument('--no-backup', action='store_true',
                       help='ä¸åˆ›å»ºå¤‡ä»½æ–‡ä»¶ (ä»…åœ¨è¦†ç›–æ¨¡å¼ä¸‹æœ‰æ•ˆ)')
    
    # å•æ–‡ä»¶æ¨¡å¼
    parser.add_argument('--input-file', '-i',
                       help='å•ä¸ªè¾“å…¥æ–‡ä»¶')
    parser.add_argument('--output-file', '-o',
                       help='å•ä¸ªè¾“å‡ºæ–‡ä»¶ (å¦‚ä¸æŒ‡å®šåˆ™è¦†ç›–åŸæ–‡ä»¶)')
    
    args = parser.parse_args()
    
    try:
        # åˆ›å»ºç¿»è¯‘å™¨
        localizer = JSONJapaneseLocalizer(
            gemini_api_key=args.api_key,
            gemini_base_url=args.base_url,
            gemini_model=args.model,
            batch_size=args.batch_size,
            max_workers=args.max_workers,
            requests_per_minute=args.rpm
        )
        
        # å•æ–‡ä»¶æˆ–ç›®å½•æ¨¡å¼
        if args.input_file:
            if not os.path.exists(args.input_file):
                print(f"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {args.input_file}")
                return
            
            print("ğŸ¯ å•æ–‡ä»¶ç¿»è¯‘æ¨¡å¼")
            create_backup = not args.no_backup
            localizer.translate_json_file(args.input_file, args.output_file, create_backup)
        else:
            print("ğŸ“ ç›®å½•æ‰¹é‡ç¿»è¯‘æ¨¡å¼")
            create_backup = not args.no_backup
            localizer.translate_directory(args.directory, args.output_suffix, create_backup)
            
    except KeyboardInterrupt:
        print(f"\nâ¹ï¸ ç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"âŒ ç¨‹åºè¿è¡Œé”™è¯¯: {e}")

if __name__ == "__main__":
    main()
